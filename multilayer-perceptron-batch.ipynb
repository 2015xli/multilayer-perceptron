{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Activation:\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    def linear(self, x):\n",
    "        return x\n",
    "    def softplus(self, x):\n",
    "        return np.log(1+np.exp(x))\n",
    "    def softmax(self, x):\n",
    "        y = np.empty(shape=x.shape)\n",
    "        for i in range(y.shape[0]):\n",
    "            exps = np.exp(x[i] - x[i].max())\n",
    "            y[i] = exps / np.sum(exps)\n",
    "            \n",
    "        return y       \n",
    "        #only softmax is not element-wise in act functions\n",
    "        #exps = np.exp(x - x.max())\n",
    "        #return exps / np.sum(exps)\n",
    "\n",
    "    def g_relu(self, x):\n",
    "        return 1 * (x > 0)\n",
    "    def g_sigmoid(self, x):\n",
    "        return (1 - x) * x\n",
    "    def g_tanh(self, x):\n",
    "        return 1 - x*x\n",
    "    def g_linear(self, x):\n",
    "        return 1 * (x==x)\n",
    "    def g_softplus(self, x):\n",
    "        return self.sigmoid(x)\n",
    "    def g_softmax(self, x):\n",
    "        y = np.empty(shape=x.shape)\n",
    "        for i in range(y.shape[0]):\n",
    "            dx_ds = np.diag(x[i]) - np.dot(x[i].T, x[i])\n",
    "            y[i] = dx_ds.sum(axis=0)\n",
    "        \n",
    "        return y \n",
    "        #only softmax is not element-wise in act functions\n",
    "        #dx_ds = np.diag(x) - np.dot(x.T, x)\n",
    "        #return dx_ds.sum(axis=0) \n",
    "    \n",
    "    def __init__(self, act):\n",
    "        funcs = {\n",
    "            \"TANH\" : self.tanh,\n",
    "            \"SIGMOID\" : self.sigmoid,\n",
    "            \"RELU\" : self.relu,\n",
    "            \"LINEAR\" : self.linear,\n",
    "            \"SOFTPLUS\" : self.softplus,\n",
    "            \"SOFTMAX\" : self.softmax\n",
    "        }\n",
    "\n",
    "        grads = {\n",
    "            \"TANH\" : self.g_tanh,\n",
    "            \"SIGMOID\" : self.g_sigmoid,\n",
    "            \"RELU\" : self.g_relu,\n",
    "            \"LINEAR\" : self.g_linear,\n",
    "            \"SOFTPLUS\" : self.g_softplus,\n",
    "            \"SOFTMAX\" : self.g_softmax\n",
    "        }\n",
    "        self.act = act\n",
    "        self.func = funcs[act]\n",
    "        self.grad = grads[act]\n",
    "        \n",
    "        return       \n",
    "    \n",
    "    def __str__(self):\n",
    "        s = \"\\nActivation:\" + self.act\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class LinearLayer:\n",
    "\n",
    "    def init_weights(self, num):\n",
    "        w=[]\n",
    "        for i in range(num): \n",
    "            w.append(random.uniform(-1, 1))\n",
    "        return w            \n",
    "\n",
    "    # W.shape = (n_node, n_in+1)\n",
    "    def __init__(self, n_in, n_node, W=None):\n",
    "        self.n_node = n_node\n",
    "        self.n_in = n_in\n",
    "        # W includes b, so the input size is n_in + 1 (i.e., x0==1) \n",
    "        size = n_node * (n_in+1)\n",
    "        if W == None:\n",
    "            weights = self.init_weights(size)\n",
    "            weights = np.array(weights).reshape(n_node, n_in+1)\n",
    "            scale = np.sqrt(2./size)\n",
    "            self.W = weights * scale\n",
    "        else: \n",
    "            nn, ni = W.shape\n",
    "            if nn != n_node or ni != n_in+1:\n",
    "                raise ValueError(\"Incorrect weights input\")\n",
    "            else:\n",
    "                self.W = W  # W.shape = (n_node, n_in+1)\n",
    "        \n",
    "        # seperately initialize b, sometimes benefiting RELU\n",
    "        # self.W[:,n_in] = 0.1 \n",
    "        # self.W[:,n_in] = np.zeros(n_node)\n",
    "        \n",
    "        self.X = None       # X.shape = (n_sample, n_in+1)\n",
    "        self.signal = None  # signal.shape = (n_sample, n_node) = g_signal.shape\n",
    "        self.Y = None       # Y.shape = (n_sample, n_node)\n",
    "\n",
    "        self.g_Y = None     # g_Y.shape = (n_sample, n_node)\n",
    "        self.g_W = None       # G.shape = (n_in+1, n_node)\n",
    "        self.g_X = None     # g_X.shape = (n_sample, n_in)\n",
    "        return\n",
    "    \n",
    "    # input: X.shape = (n_sample, n_in)\n",
    "    def forward(self, X):\n",
    "        n_sample, n_in = X.shape\n",
    "        self.X = np.ones(shape=(n_sample, n_in+1))\n",
    "        self.X[:,:n_in] = X  # add column (x0=1)\n",
    "        self.signal = np.dot(self.X, self.W.T)\n",
    "        return self.signal\n",
    "\n",
    "    # input: g_signal.shape = (n_sample, n_node)\n",
    "    def backward(self, g_signal):\n",
    "        n_sample, n_node = g_signal.shape\n",
    "        self.g_W = np.dot(self.X.T, g_signal)/n_sample\n",
    "        g_X = np.dot(g_signal, self.W)  # W.shape = (n_sample, n_in+1)\n",
    "        self.g_X = g_X[:,:self.n_in]  # remove column (meaningless gradient to x0=1)\n",
    "        return self.g_X\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        #set_trace()\n",
    "        self.W = self.W - self.g_W.T * learning_rate\n",
    "        return\n",
    "            \n",
    "    def __str__(self):\n",
    "        s = \"\\nX is:\\n\"+str(self.X)\n",
    "        s += \"\\nW is:\\n\" + str(self.W)\n",
    "        s += \"\\nY is:\\n\"+str(self.Y)\n",
    "        s += \"\\ng_Y is:\\n\"+str(self.g_Y)\n",
    "        s += \"\\ng_W is:\\n\"+str(self.g_W)\n",
    "        s += \"\\ng_X is:\\n\"+str(self.g_X)\n",
    "        return s\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuronLayer(LinearLayer):\n",
    "        \n",
    "    def __init__(self, n_in, n_node, W=None, act=\"RELU\"):\n",
    "        super().__init__(n_in, n_node, W=None)\n",
    "        self.activation = Activation(act)\n",
    "        return\n",
    "        \n",
    "    def forward(self, X):\n",
    "        n_sample,n_in = X.shape\n",
    "        if n_in != self.n_in: \n",
    "            raise ValueError(\"Incorrect data input\", n_in, self.n_in)\n",
    "        # linear part first\n",
    "        signal = super().forward(X)\n",
    "        # nonlinear\n",
    "        self.Y = self.activation.func(signal) \n",
    "        \n",
    "        return self.Y\n",
    "      \n",
    "    def backward(self, g_Y):\n",
    "        n_sample, n_node = g_Y.shape\n",
    "        if n_node != self.n_node: \n",
    "            raise ValueError(\"Incorrect data input\")\n",
    "        self.g_Y = g_Y\n",
    "\n",
    "        # nonlinear\n",
    "        g_signal = self.activation.grad(self.Y)\n",
    "        g_signal = g_signal * g_Y\n",
    "        \n",
    "        # linear\n",
    "        self.g_X = super().backward(g_signal)\n",
    "        return self.g_X\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        super().update(learning_rate)\n",
    "        return\n",
    "\n",
    "    def __str__(self):\n",
    "        s = super().__str__()\n",
    "        s += str(self.activation)\n",
    "        return s\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftMaxLayer(NeuronLayer):\n",
    "        \n",
    "    def __init__(self, n_in, n_node, W=None):\n",
    "        super().__init__(n_in, n_node, W=None, act=\"SOFTMAX\")\n",
    "        self.predict = None\n",
    "        self.truth = None\n",
    "        return\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # hidden part first, computing softmax\n",
    "        self.Y = super().forward(X)\n",
    "        self.predict = self.Y.argmax(axis=1)\n",
    "        return self.predict\n",
    "      \n",
    "    def backward(self, T):\n",
    "        # softmax gradient, dL/ds (combining the cost and activation layer)\n",
    "        n_sample = T.size\n",
    "        self.truth = np.zeros(shape=(n_sample, self.n_node))\n",
    "        self.truth[np.arange(n_sample), T] = 1\n",
    "        g_signal = (self.Y - self.truth)\n",
    "        # then linear layer\n",
    "        self.g_X = LinearLayer.backward(self, g_signal)\n",
    "        return self.g_X\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        super().update(learning_rate)\n",
    "        return\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = super().__str__()\n",
    "        s += \"\\nPrediction:\\n\" + str(self.predict)\n",
    "        s += \"\\nTruth:\\n\" + str(self.truth)\n",
    "        return s\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, n_in, n_node, n_layer=1, output_layer=None):   \n",
    "        self.n_in = n_in\n",
    "        self.n_node = n_node if (n_node != None) else (n_in+1)\n",
    "        if n_layer == 0:\n",
    "            raise ValueError(\"n_layer should not be 0.\") \n",
    "\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_layer = list() \n",
    "        self.hidden_layer.append(NeuronLayer(n_in, n_node))\n",
    "        for i in range(1, n_layer):\n",
    "            self.hidden_layer.append(NeuronLayer(n_node, n_node))\n",
    "\n",
    "        if output_layer == None:\n",
    "            raise ValueError(\"Output_layer should be assigned.\") \n",
    "        \n",
    "        self.output_layer = output_layer\n",
    "        self.learning_rate = 0.1\n",
    "        return\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #set_trace()\n",
    "        for i in range(self.n_layer):\n",
    "            X = self.hidden_layer[i].forward(X)\n",
    "\n",
    "        self.output_layer.forward(X)\n",
    "        return\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        #set_trace()\n",
    "        g_Y = self.output_layer.backward(Y)\n",
    "        for i in range(self.n_layer-1, -1, -1):\n",
    "            g_Y = self.hidden_layer[i].backward(g_Y)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def update(self):\n",
    "        for i in range(self.n_layer):\n",
    "            self.hidden_layer[i].update(self.learning_rate)\n",
    "\n",
    "        self.output_layer.update(self.learning_rate)\n",
    "        return\n",
    "    \n",
    "    def train_1batch(self, X, Y, learning_rate=0.1):\n",
    "        if learning_rate != 0:\n",
    "            self.learning_rate = learning_rate\n",
    "            \n",
    "        self.forward(X)\n",
    "        self.backward(Y)\n",
    "        self.update()\n",
    "        return\n",
    "    \n",
    "    def predict_1sample(self, x):\n",
    "        #set_trace()\n",
    "        for i in range(self.n_layer):\n",
    "            x = self.hidden_layer[i].forward(x)\n",
    "\n",
    "        predict = self.output_layer.forward(x)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pre-requirement: MNIST files from http://yann.lecun.com/exdb/mnist/ stored in local directory\n",
    "class MnistInput:\n",
    "    def __init__(self, data):\n",
    "        if data == \"training\":\n",
    "            zX = './train-images-idx3-ubyte.gz'\n",
    "            zy = './train-labels-idx1-ubyte.gz'\n",
    "        elif data == \"testing\":\n",
    "            zX = './t10k-images-idx3-ubyte.gz'\n",
    "            zy = './t10k-labels-idx1-ubyte.gz'\n",
    "        else: raise ValueError(\"Incorrect data input\")\n",
    "        \n",
    "        self.zX = zX\n",
    "        self.zy = zy\n",
    "        return\n",
    "    \n",
    "    def read(self, num):\n",
    "\n",
    "        zX = self.zX\n",
    "        zy = self.zy\n",
    "        with gzip.open(zX) as fX, gzip.open(zy) as fy:\n",
    "            magic, nX, rows, cols = struct.unpack(\">IIII\", fX.read(16))\n",
    "            magic, ny = struct.unpack(\">II\", fy.read(8))\n",
    "            if nX != ny: raise ValueError(\"Inconsistent data and label files\")\n",
    "\n",
    "            img_size = cols*rows\n",
    "            if num <= 0 or num > nX: num = nX \n",
    "            for i in range(num):\n",
    "                X = struct.unpack(\"B\"*img_size, fX.read(img_size))\n",
    "                y, = struct.unpack(\"B\", fy.read(1))\n",
    "                yield (X, y)\n",
    "        return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "class MNIST:\n",
    "\n",
    "    def __init__(self, nn):\n",
    "        self.nn = nn\n",
    "        self.train_input = MnistInput(\"training\")\n",
    "        self.test_input = MnistInput(\"testing\")\n",
    "        self.n_epoch = 0\n",
    "        return\n",
    "       \n",
    "    def train_in_batch(self, n_sample, batch_size=100, epochs=500, learning_rate=0.1):\n",
    "        learning_rate /= (1 + self.n_epoch*0.1)\n",
    "        self.n_epoch += 1\n",
    "        n_ep = 0\n",
    "        n_x = 0 \n",
    "        X = []\n",
    "        Y = []\n",
    "        batch_size = batch_size if batch_size > 0 else 100\n",
    "        epochs = epochs if epochs > 0 else 500\n",
    "        for x, y in self.train_input.read(n_sample):\n",
    "            n_x += 1\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            if n_x >= batch_size:\n",
    "                n_ep += 1\n",
    "                X = np.array(X)/255\n",
    "                Y = np.array(Y)\n",
    "                self.nn.train_1batch(X, Y, learning_rate)\n",
    "                if  n_ep >= epochs: return\n",
    "\n",
    "                n_x = 0\n",
    "                X = []\n",
    "                Y = []                \n",
    "        return\n",
    "    \n",
    "    def test(self, n_sample):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, y in self.test_input.read(n_sample):\n",
    "            X = np.array(x).reshape(1, len(x)) / 255\n",
    "            predict = self.nn.predict_1sample(X)\n",
    "            correct += 1 * (predict[0] == y)\n",
    "            total += 1\n",
    "            #print(\"\\nPredict:\", predict)\n",
    "            #plt.imshow(x, cmap=mpl.cm.Greys)\n",
    "            #plt.show()\n",
    "        accuracy = correct/total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_mnist():\n",
    "    output_layer = SoftMaxLayer(28*28+1, 10)\n",
    "    mlp = MLP(28*28, 28*28+1, output_layer = output_layer)\n",
    "    mnist = MNIST(mlp)\n",
    "    for i in range(10):\n",
    "        mnist.train_in_batch(-1,100,500,1)\n",
    "        accuracy = mnist.test(-1)\n",
    "        print(\"\\nEpoch {} accuracy: {}\".format(i, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# learn the index of number \"3\".\n",
    "def run_test():\n",
    "    out = SoftMaxLayer(4,3)\n",
    "    mlp = MLP(3,4, output_layer = out)\n",
    "    X = [\n",
    "        [1,2,3],\n",
    "        [2,1,3],\n",
    "        [3,1,2],\n",
    "        [3,2,1],\n",
    "        [1,3,2],\n",
    "        [2,3,1]\n",
    "        ]\n",
    "    Y = [2,2,0,0,1,1]\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    for j in range(160):\n",
    "        mlp.train_1batch(X, Y)\n",
    "\n",
    "    '''\n",
    "    for j in range(40):\n",
    "        for i in range(X.shape[0]):\n",
    "            mlp.train_1batch(np.array([X[i]]), np.array([Y[i]]))\n",
    "    '''\n",
    "\n",
    "    X = np.array([[1,2,3],[2,3,1],[3,2,2]])\n",
    "    for i in range(X.shape[0]):\n",
    "        predict = mlp.predict_1sample(np.array([X[i]]))\n",
    "        print(\"\\nPredict: \", X[i], predict[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_main_module():\n",
    "    return __name__ == '__main__' and '__file__' not in globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predict:  [1 2 3] 2\n",
      "\n",
      "Predict:  [2 3 1] 1\n",
      "\n",
      "Predict:  [3 2 2] 0\n",
      "\n",
      "Epoch 0 accuracy: 0.9561\n",
      "\n",
      "Epoch 1 accuracy: 0.9643\n",
      "\n",
      "Epoch 2 accuracy: 0.9708\n",
      "\n",
      "Epoch 3 accuracy: 0.9752\n",
      "\n",
      "Epoch 4 accuracy: 0.9761\n",
      "\n",
      "Epoch 5 accuracy: 0.9769\n",
      "\n",
      "Epoch 6 accuracy: 0.9775\n"
     ]
    }
   ],
   "source": [
    "if is_main_module():\n",
    "    run_test()\n",
    "    run_mnist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
